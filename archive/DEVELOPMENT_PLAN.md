# AgentPrivacyBench (APB) - Plan de DÃ©veloppement ItÃ©ratif

> **Objectif** : Construire le benchmark APB de maniÃ¨re incrÃ©mentale, en validant chaque composant avant de passer au suivant.

## ğŸ¯ Target State (Paper)

Le paper dÃ©crit un benchmark avec :
- **1000 scÃ©narios** (250 par vertical : Healthcare, Finance, Legal, Corporate)
- **15 classes d'attaques** en 4 familles
- **7 canaux de leakage** (C1-C7)
- **3 niveaux d'adversaires** (A0, A1, A2)
- **Harness framework-agnostic** avec adapters
- **MÃ©triques** : ELR, WLS, CLR, ASR, Pareto AUC

---

## ğŸ“‹ Phases de DÃ©veloppement

### **Phase 0 : Infrastructure de Base (Semaine 1)**
```
Objectif: Setup projet, CI/CD, structure de base
```

#### TÃ¢ches :
- [ ] **0.1** CrÃ©er structure de projet Python (`apb/`)
- [ ] **0.2** Setup environnement (pyproject.toml, requirements.txt)
- [ ] **0.3** CrÃ©er schÃ©mas Pydantic pour les scÃ©narios
- [ ] **0.4** Tests unitaires de base
- [ ] **0.5** GitHub Actions CI

#### Validation :
```bash
pytest tests/ -v  # Tous les tests passent
python -c "from apb import Scenario; print('OK')"
```

---

### **Phase 1 : GÃ©nÃ©ration de ScÃ©narios (Semaines 2-3)**
```
Objectif: GÃ©nÃ©rer 100 scÃ©narios de validation (APB-Lite)
```

#### TÃ¢ches :
- [ ] **1.1** CrÃ©er templates de scÃ©narios par vertical (JSON)
- [ ] **1.2** ImplÃ©menter gÃ©nÃ©rateur de donnÃ©es synthÃ©tiques (Faker)
- [ ] **1.3** CrÃ©er systÃ¨me de canaries (3 tiers: obvious/realistic/semantic)
- [ ] **1.4** DÃ©finir `allowed_set` et `private_vault` pour chaque template
- [ ] **1.5** Script de gÃ©nÃ©ration en batch avec LLM pour variÃ©tÃ©
- [ ] **1.6** Validation humaine de 20 scÃ©narios

#### Livrables :
- `apb/generators/scenario_generator.py`
- `apb/data/scenarios/apb_lite_100.jsonl`
- `apb/schemas/scenario.py`

#### Validation :
```bash
python -m apb.generators.scenario_generator --count 25 --vertical healthcare
# VÃ©rifier que les scÃ©narios sont cohÃ©rents
```

---

### **Phase 2 : Taxonomie d'Attaques (Semaines 3-4)**
```
Objectif: ImplÃ©menter les 15 classes d'attaques
```

#### TÃ¢ches :
- [ ] **2.1** CrÃ©er base abstraite `Attack` avec interface commune
- [ ] **2.2** ImplÃ©menter Family 1 (4 attaques prompt/instruction)
- [ ] **2.3** ImplÃ©menter Family 2 (4 attaques tool-surface)
- [ ] **2.4** ImplÃ©menter Family 3 (4 attaques memory/persistence)
- [ ] **2.5** ImplÃ©menter Family 4 (3 attaques multi-agent)
- [ ] **2.6** CrÃ©er payloads templates paramÃ©trables
- [ ] **2.7** Tests unitaires pour chaque classe d'attaque

#### Livrables :
- `apb/attacks/base.py`
- `apb/attacks/family1_prompt.py`
- `apb/attacks/family2_tool.py`
- `apb/attacks/family3_memory.py`
- `apb/attacks/family4_multiagent.py`
- `apb/attacks/payloads/` (templates YAML)

#### Validation :
```bash
python -m apb.attacks.test_all  # 15 classes fonctionnelles
```

---

### **Phase 3 : Harness d'Ã‰valuation (Semaines 4-5)**
```
Objectif: CrÃ©er le harness framework-agnostic avec trace JSONL
```

#### TÃ¢ches :
- [ ] **3.1** DÃ©finir format de trace JSONL unifiÃ©
- [ ] **3.2** CrÃ©er interface `FrameworkAdapter` (4 mÃ©thodes)
- [ ] **3.3** ImplÃ©menter adapter LangChain (prioritaire)
- [ ] **3.4** ImplÃ©menter adapter OpenAI API (custom)
- [ ] **3.5** CrÃ©er mock tools (case_db, email, calendar, etc.)
- [ ] **3.6** Runner de scÃ©narios avec logging

#### Livrables :
- `apb/harness/trace.py`
- `apb/harness/adapter_base.py`
- `apb/harness/adapters/langchain_adapter.py`
- `apb/harness/adapters/openai_adapter.py`
- `apb/harness/tools/` (mock tools)
- `apb/harness/runner.py`

#### Validation :
```bash
python -m apb.harness.runner --scenario apb_fin_001 --framework langchain
# VÃ©rifier trace JSONL gÃ©nÃ©rÃ©e correctement
```

---

### **Phase 4 : Pipeline de DÃ©tection (Semaines 5-6)**
```
Objectif: ImplÃ©menter dÃ©tection de leakage 3-stages
```

#### TÃ¢ches :
- [ ] **4.1** Stage 1: Exact canary matching (regex)
- [ ] **4.2** Stage 2: Structured field audit (patterns SSN, CC, etc.)
- [ ] **4.3** Stage 3: Semantic similarity (embeddings + threshold)
- [ ] **4.4** Calibration du seuil Ï„ sur set de validation
- [ ] **4.5** Calcul matrice de confusion
- [ ] **4.6** Support per-channel detection (C1-C7)

#### Livrables :
- `apb/detection/canary_matcher.py`
- `apb/detection/field_auditor.py`
- `apb/detection/semantic_detector.py`
- `apb/detection/pipeline.py`

#### Validation :
```bash
python -m apb.detection.pipeline --trace trace.jsonl --scenario apb_fin_001
# Output: {"leaked": true, "channels": ["C3", "C5"], "fields": ["ssn"]}
```

---

### **Phase 5 : MÃ©triques (Semaine 6)**
```
Objectif: ImplÃ©menter ELR, WLS, CLR, ASR, Pareto AUC
```

#### TÃ¢ches :
- [ ] **5.1** ImplÃ©menter ELR (Exact Leakage Rate)
- [ ] **5.2** ImplÃ©menter WLS (Weighted Leakage Score)
- [ ] **5.3** ImplÃ©menter CLR per-channel
- [ ] **5.4** ImplÃ©menter ASR (Attack Success Rate)
- [ ] **5.5** ImplÃ©menter TSR (Task Success Rate) avec oracles
- [ ] **5.6** Calculer Pareto AUC et dominance rate

#### Livrables :
- `apb/metrics/leakage.py`
- `apb/metrics/utility.py`
- `apb/metrics/pareto.py`
- `apb/metrics/report.py`

#### Validation :
```bash
python -m apb.metrics.report --results results.jsonl
# Output: ELR=0.68, WLS=2.31, TSR=0.87, Pareto_AUC=0.45
```

---

### **Phase 6 : IntÃ©gration DÃ©fenses (Semaine 7)**
```
Objectif: IntÃ©grer LCF et baselines de dÃ©fense
```

#### TÃ¢ches :
- [ ] **6.1** Wrapper pour output filtering (regex PII)
- [ ] **6.2** Wrapper pour policy prompt injection
- [ ] **6.3** IntÃ©gration LCF (from paper3)
- [ ] **6.4** Benchmark comparatif des dÃ©fenses

#### Livrables :
- `apb/defenses/output_filter.py`
- `apb/defenses/policy_prompt.py`
- `apb/defenses/lcf_wrapper.py`

---

### **Phase 7 : Scale Up + Leaderboard (Semaines 8-10)**
```
Objectif: Passer de 100 Ã  1000 scÃ©narios, crÃ©er leaderboard
```

#### TÃ¢ches :
- [ ] **7.1** GÃ©nÃ©ration des 1000 scÃ©narios complets
- [ ] **7.2** Validation humaine (sample 50)
- [ ] **7.3** Split 700/300 public/private
- [ ] **7.4** Setup leaderboard (Streamlit ou HuggingFace Spaces)
- [ ] **7.5** Documentation complÃ¨te

---

## ğŸ—‚ï¸ Structure du Code

```
paper4/
â”œâ”€â”€ apb/                          # Package principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ scenario.py           # Pydantic models
â”‚   â”‚   â”œâ”€â”€ trace.py              # Trace event models
â”‚   â”‚   â””â”€â”€ attack.py             # Attack config models
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â”œâ”€â”€ scenario_generator.py # GÃ©nÃ©ration scÃ©narios
â”‚   â”‚   â”œâ”€â”€ canary_generator.py   # 3-tier canaries
â”‚   â”‚   â””â”€â”€ vault_generator.py    # Private vaults
â”‚   â”œâ”€â”€ attacks/
â”‚   â”‚   â”œâ”€â”€ base.py               # Abstract Attack class
â”‚   â”‚   â”œâ”€â”€ family1_prompt.py     # DPI, Role Confusion, etc.
â”‚   â”‚   â”œâ”€â”€ family2_tool.py       # IPI, Tool Poisoning, etc.
â”‚   â”‚   â”œâ”€â”€ family3_memory.py     # Memory exfil, logs, etc.
â”‚   â”‚   â”œâ”€â”€ family4_multiagent.py # Cross-agent, delegation
â”‚   â”‚   â””â”€â”€ payloads/             # YAML templates
â”‚   â”œâ”€â”€ harness/
â”‚   â”‚   â”œâ”€â”€ adapter_base.py       # FrameworkAdapter interface
â”‚   â”‚   â”œâ”€â”€ adapters/
â”‚   â”‚   â”‚   â”œâ”€â”€ langchain_adapter.py
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_adapter.py
â”‚   â”‚   â”‚   â””â”€â”€ crewai_adapter.py
â”‚   â”‚   â”œâ”€â”€ tools/                # Mock tools
â”‚   â”‚   â”‚   â”œâ”€â”€ case_db.py
â”‚   â”‚   â”‚   â”œâ”€â”€ email.py
â”‚   â”‚   â”‚   â””â”€â”€ calendar.py
â”‚   â”‚   â”œâ”€â”€ trace.py              # JSONL trace writer
â”‚   â”‚   â””â”€â”€ runner.py             # Scenario executor
â”‚   â”œâ”€â”€ detection/
â”‚   â”‚   â”œâ”€â”€ canary_matcher.py     # Stage 1: exact match
â”‚   â”‚   â”œâ”€â”€ field_auditor.py      # Stage 2: patterns
â”‚   â”‚   â”œâ”€â”€ semantic_detector.py  # Stage 3: embeddings
â”‚   â”‚   â””â”€â”€ pipeline.py           # Combined pipeline
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ leakage.py            # ELR, WLS, CLR, ASR
â”‚   â”‚   â”œâ”€â”€ utility.py            # TSR, cost
â”‚   â”‚   â”œâ”€â”€ pareto.py             # Pareto AUC, dominance
â”‚   â”‚   â””â”€â”€ report.py             # Aggregate reporting
â”‚   â”œâ”€â”€ defenses/
â”‚   â”‚   â”œâ”€â”€ output_filter.py
â”‚   â”‚   â”œâ”€â”€ policy_prompt.py
â”‚   â”‚   â””â”€â”€ lcf_wrapper.py
â”‚   â””â”€â”€ cli/
â”‚       â”œâ”€â”€ generate.py           # apb generate
â”‚       â”œâ”€â”€ run.py                # apb run
â”‚       â””â”€â”€ evaluate.py           # apb evaluate
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ scenarios/
â”‚   â”‚   â”œâ”€â”€ apb_lite_100.jsonl    # 100 scÃ©narios pour dev
â”‚   â”‚   â””â”€â”€ apb_full_1000.jsonl   # 1000 scÃ©narios complets
â”‚   â”œâ”€â”€ payloads/                 # Attack payloads
â”‚   â””â”€â”€ calibration/              # Threshold calibration data
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_schemas.py
â”‚   â”œâ”€â”€ test_generators.py
â”‚   â”œâ”€â”€ test_attacks.py
â”‚   â”œâ”€â”€ test_harness.py
â”‚   â”œâ”€â”€ test_detection.py
â”‚   â””â”€â”€ test_metrics.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_scenario_exploration.ipynb
â”‚   â”œâ”€â”€ 02_attack_testing.ipynb
â”‚   â””â”€â”€ 03_results_analysis.ipynb
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Commencer Maintenant (Phase 0)

### Ã‰tape 1 : CrÃ©er la structure de base

```bash
cd paper4
mkdir -p apb/{schemas,generators,attacks,harness/adapters,harness/tools,detection,metrics,defenses,cli}
mkdir -p data/{scenarios,payloads,calibration}
mkdir -p tests notebooks docs
touch apb/__init__.py
```

### Ã‰tape 2 : CrÃ©er les schÃ©mas Pydantic

Commencer par `apb/schemas/scenario.py` - le cÅ“ur du benchmark.

### Ã‰tape 3 : Premier test

```bash
python -c "from apb.schemas.scenario import Scenario; print('Structure OK')"
```

---

## ğŸ“Š CritÃ¨res de SuccÃ¨s par Phase

| Phase | CritÃ¨re | MÃ©trique |
|-------|---------|----------|
| 0 | Tests passent | 100% pytest |
| 1 | 100 scÃ©narios valides | Validation JSON Schema |
| 2 | 15 attaques fonctionnelles | Tests unitaires |
| 3 | Traces JSONL correctes | 1 scÃ©nario end-to-end |
| 4 | FNR < 10% sur validation | Matrice confusion |
| 5 | MÃ©triques correctes | Comparaison manuelle |
| 6 | LCF intÃ©grÃ© | Pareto plot gÃ©nÃ©rÃ© |
| 7 | 1000 scÃ©narios | Leaderboard live |

---

## ğŸ”„ Cycle d'ItÃ©ration

```
Pour chaque phase:
1. ImplÃ©menter le minimum viable
2. Ã‰crire les tests
3. Valider sur 5-10 exemples
4. Documenter les limitations
5. Commit + tag version
6. Passer Ã  la phase suivante
```

---

## âš¡ Quick Wins (Ordre de prioritÃ©)

1. **Schemas Pydantic** â†’ Validation automatique
2. **1 scÃ©nario complet** â†’ Preuve de concept
3. **1 attaque simple (DPI)** â†’ Pipeline de bout en bout
4. **DÃ©tection canary exacte** â†’ Baseline mÃ©triques
5. **LangChain adapter** â†’ Framework le plus populaire

---

## ğŸ“… Timeline RÃ©aliste

| Semaine | Livrables |
|---------|-----------|
| 1 | Structure + Schemas + 10 scÃ©narios |
| 2-3 | 100 scÃ©narios + GÃ©nÃ©rateurs |
| 3-4 | 15 attaques implÃ©mentÃ©es |
| 4-5 | Harness + LangChain adapter |
| 5-6 | Detection pipeline + MÃ©triques |
| 7 | DÃ©fenses + LCF |
| 8-10 | Scale 1000 + Leaderboard |

**Total estimÃ© : 8-10 semaines pour MVP complet**
